{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Temperature and US Immigration\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "For this project, we will have to aggregate the I94 immigration data by destination city to form our first-dimension table. We will then aggregate city temperature data by city to create the second-dimension table. Both tables will be joined on destination city to form the fact table. We will be using Spark to process the data.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "The I94 immigration data comes from the US National Tourism and Trade Office website. It is provided in SAS7BDAT.\n",
    "\n",
    "**Key Notes:**\n",
    "- i94yr = 4 digit year\n",
    "- i94mon = numeric month\n",
    "- i94cit = 3 digit code of origin city\n",
    "- i94port = 3 character code of destination USA city\n",
    "- arrdate = arrival date in the USA\n",
    "- i94mode = 1 digit travel code\n",
    "- depdate = departure date from the USA\n",
    "- i94visa = reason for immigration\n",
    "\n",
    "The temperature data set [Kaggle](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/). Csv format.\n",
    "\n",
    "**Notes**\n",
    "- AverageTemperature = average temperature\n",
    "- City = city name\n",
    "- Country = country name\n",
    "- Latitude= latitude\n",
    "- Longitude = longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Set libraries and create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Installed all dependencies\n",
    "import re\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create una sesion de spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "            .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "            .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take and read immigration data for April 16\n",
    "#file_name_immigration = 'data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "file_name_immigration = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_immigration = pd.read_sas(file_name_immigration, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Show immigration data\n",
    "df_immigration.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Ã…rhus   \n",
       "1  1743-12-01                 NaN                            NaN  Ã…rhus   \n",
       "2  1744-01-01                 NaN                            NaN  Ã…rhus   \n",
       "3  1744-02-01                 NaN                            NaN  Ã…rhus   \n",
       "4  1744-03-01                 NaN                            NaN  Ã…rhus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take and read global temperature data\n",
    "#file_name_temperature = 'data2/GlobalLandTemperaturesByCity.csv'\n",
    "file_name_temperature = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature = pd.read_csv(file_name_temperature, sep=',')\n",
    "\n",
    "# Displays temperature data\n",
    "df_temperature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "#### Explore the Data \n",
    "\n",
    "<b>I94 immigration data</b> - drop all data points with the destination city code i94port is not a valid value like (XXX, 99, NaN, etc). \n",
    "\n",
    "<b>Temperature Data</b> - drop all data points where AverageTemperature is NaN, duplicate locations, and add the i94port of the location in each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Take valid port values by using regular expressions\n",
    "reg_exp_port = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "port_valid = {}\n",
    "with open('I94port_valid.txt') as f:\n",
    "    for line in f:\n",
    "        match = reg_exp_port.search(line)\n",
    "        port_valid[match[1]]=[match[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Take immigration data for valid ports\n",
    "def clean_immigration_data(file):\n",
    "    '''\n",
    "    This function reads immigration data using the sas format with spark.\n",
    "    Then, filter and match the immigration data.\n",
    "    \n",
    "    Input:\n",
    "    * file the immigration file in sas format\n",
    "    \n",
    "    Output:\n",
    "    * df_immigration the filtered dataframe with the ports valid data\n",
    "    '''\n",
    "    \n",
    "    # read the file in sas format\n",
    "    df_immigration = spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "    \n",
    "    # filter and match the immigration data\n",
    "    df_immigration = df_immigration.filter(df_immigration.i94port.isin(list(port_valid.keys())))\n",
    "    \n",
    "    return df_immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf()\n",
    "def get_key_port(city):\n",
    "    '''\n",
    "    This function compares a city belonging to the list of valid ports.\n",
    "    \n",
    "    Input:\n",
    "    * city the city name\n",
    "    \n",
    "    Output:\n",
    "    * key the corresponding key port\n",
    "    '''\n",
    "    \n",
    "    # Evaluate the city port by port\n",
    "    for key in port_valid:\n",
    "        if city.lower() in port_valid[key][0].lower():\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------------------------+---------+--------------------+--------+---------+-------+\n",
      "|        dt| AverageTemperature|AverageTemperatureUncertainty|     City|             Country|Latitude|Longitude|i94port|\n",
      "+----------+-------------------+-----------------------------+---------+--------------------+--------+---------+-------+\n",
      "|1852-07-01|             15.488|                        1.395|    Perth|           Australia|  31.35S|  114.97E|    PER|\n",
      "|1828-01-01|             -1.977|                        2.551|  Seattle|       United States|  47.42N|  121.97W|    SEA|\n",
      "|1743-11-01|              2.767|                        1.905| Hamilton|              Canada|  42.59N|   80.73W|    HAM|\n",
      "|1849-01-01|  7.399999999999999|                        2.699|  Ontario|       United States|  34.56N|  116.76W|    ONT|\n",
      "|1821-11-01|              2.322|                        2.375|  Spokane|       United States|  47.42N|  117.24W|    SPO|\n",
      "|1843-01-01| 18.874000000000002|                        2.017|Abu Dhabi|United Arab Emirates|  24.92N|   54.98E|    MAA|\n",
      "|1824-01-01|             25.229|                        1.094|    Anaco|           Venezuela|   8.84N|   64.05W|    ANA|\n",
      "|1855-05-01|              9.904|           1.4369999999999998|      Ica|                Peru|  13.66S|   75.14W|    CHI|\n",
      "|1835-01-01|              9.833|                        2.182|  Nogales|       United States|  31.35N|  111.20W|    NOG|\n",
      "|1743-11-01|  8.129999999999999|                        2.245|  Atlanta|       United States|  34.56N|   83.68W|    ATL|\n",
      "|1796-01-01|             15.552|                        2.305|      Mau|               India|  26.52N|   84.18E|    OGG|\n",
      "|1743-11-01|              3.264|                        1.665|   Newark|       United States|  40.99N|   74.56W|    NEW|\n",
      "|1857-01-01| 18.581000000000003|           1.8119999999999998|  Springs|        South Africa|  26.52S|   28.66E|    PSP|\n",
      "|1856-01-01| 26.055999999999997|           1.3769999999999998|      Ise|             Nigeria|   7.23N|    5.68E|    BOI|\n",
      "|1743-11-01|             18.722|                        2.302|  Orlando|       United States|  28.13N|   80.91W|    ORL|\n",
      "|1823-01-01|             11.602|           2.8160000000000003|   Laredo|       United States|  28.13N|   99.09W|    LCB|\n",
      "|1841-01-01| 13.107999999999999|                        2.519|     Tali|              Taiwan|  24.92N|  120.59E|    MET|\n",
      "|1828-01-01|-2.7630000000000003|                        2.617| Victoria|              Canada|  49.03N|  122.45W|    VIC|\n",
      "|1743-11-01| 1.1880000000000002|                        1.531|   Boston|       United States|  42.59N|   72.00W|    BOS|\n",
      "|1849-01-01|  8.091999999999999|           2.1919999999999997|Fairfield|       United States|  37.78N|  122.03W|    FTF|\n",
      "+----------+-------------------+-----------------------------+---------+--------------------+--------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load temperature data with csv format\n",
    "#df_temperature_cleaned = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data2/GlobalLandTemperaturesByCity.csv\")\n",
    "df_temperature_cleaned = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "\n",
    "# Exclude average temperatures with 'Nan' value\n",
    "df_temperature_cleaned = df_temperature_cleaned.filter(df_temperature_cleaned.AverageTemperature != 'NaN')\n",
    "\n",
    "# Exclude city and country values with duplicates\n",
    "df_temperature_cleaned = df_temperature_cleaned.dropDuplicates(['City', 'Country'])\n",
    "\n",
    "# Create i94port column \n",
    "df_temperature_cleaned = df_temperature_cleaned.withColumn(\"i94port\", get_key_port(df_temperature_cleaned.City))\n",
    "\n",
    "# Exclude ports with null value\n",
    "df_temperature_cleaned = df_temperature_cleaned.filter(df_temperature_cleaned.i94port != 'null')\n",
    "\n",
    "# Show a part of the data\n",
    "df_temperature_cleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "\n",
    "#### Data Model Justification\n",
    "\n",
    "For the present project (I94 immigrants from the USA) the star scheme will be used.\n",
    "This allows you to use fact and dimension tables to answer business questions.\n",
    "This scheme is quite common for ETL processes, however it is useful and clean to have critical information available in the fact tables, allowing flexibility in queries.\n",
    "This is an optimized scheme to answer questions like:\n",
    "What are the airports with the most immigration to the US on 94?\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "**Fact Table** - This will contain information from the I94 immigration data joined with the city temperature data on i94port\n",
    "\n",
    "Columns:\n",
    "- i94yr = 4 digit year\n",
    "- i94mon = numeric month\n",
    "- i94cit = 3 digit code of origin city\n",
    "- i94port = 3 character code of destination city\n",
    "- arrdate = arrival date\n",
    "- i94mode = 1 digit travel code\n",
    "- depdate = departure date\n",
    "- i94visa = reason for immigration\n",
    "- AverageTemperature = average temperature of destination city\n",
    "\n",
    "**1st Dimension Table** - This will contain events from the I94 immigration data.\n",
    "\n",
    "Columns:\n",
    "- i94yr = 4 digit year\n",
    "- i94mon = numeric month\n",
    "- i94cit = 3 digit code of origin city\n",
    "- i94port = 3 character code of destination city\n",
    "- arrdate = arrival date\n",
    "- i94mode = 1 digit travel code\n",
    "- depdate = departure date\n",
    "- i94visa = reason for immigration\n",
    "\n",
    "**2nd Dimension Table** - This will contain city temperature data.\n",
    "\n",
    "Columns:\n",
    "- i94port = 3 character code of destination city (mapped from cleaned up immigration data)\n",
    "- AverageTemperature = average temperature\n",
    "- City = city name\n",
    "- Country = country name\n",
    "- Latitude= latitude\n",
    "- Longitude = longitude\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "Pipeline Steps:\n",
    "1. Clean I94 data as described in step 2 to create Spark dataframe df_immigration for each month.\n",
    "2. Clean temperature data as described in step 2 to create Spark dataframe df_temperature_cleaned (this was already performed).\n",
    "3. Create immigration dimension table by selecting relevant columns from df_immigration and write to parquet file partitioned by i94port.\n",
    "4. Create temperature dimension table by selecting relevant columns from df_temperature_cleaned and write to parquet file partitioned by i94port.\n",
    "5. Create fact table by joining immigration and temperature dimension tables on i94port and write to parquet file partitioned by i94port."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "\n",
    "#### 4.1 Create the data model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# takes immigration data from April 16 in sas format\n",
    "#filename_immigration = 'data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "filename_immigration = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "\n",
    "# filter data from valid ports\n",
    "df_immigration_cleaned = clean_immigration_data(filename_immigration)\n",
    "\n",
    "# select the columns: i94yr, i94mon, i94cit, i94port, arrdate, i94mode, depdate, i94visa\n",
    "df_immigration = df_immigration_cleaned.select([\"i94yr\", \"i94mon\", \"i94cit\", \"i94port\", \"arrdate\", \"i94mode\", \"depdate\", \"i94visa\"])\n",
    "\n",
    "# write results (partitioned by i94port) in parquet format\n",
    "# complete path: /results/immigration.parquet\n",
    "df_immigration.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"results/immigration.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# select the columns: AverageTemperature, City, Country, Latitude, Longitude, i94port\n",
    "df_temperature = df_temperature_cleaned.select([\"AverageTemperature\", \"City\", \"Country\", \"Latitude\", \"Longitude\", \"i94port\"])\n",
    "\n",
    "# write temperature_table results (partitioned by i94port) in parquet format\n",
    "# complete path: /results/temperature.parquet\n",
    "df_temperature.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"results/temperature.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Step 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create temporary tables to work with sparksql\n",
    "df_immigration.createOrReplaceTempView(\"df_immigration\")\n",
    "df_temperature_cleaned.createOrReplaceTempView(\"df_temperature\")\n",
    "\n",
    "# crea tabla de hechos con spark sql\n",
    "# select the columns:   year, month, city, i94port, arrival_date, \\\n",
    "#                       departure_date, reason, temperature, latitude, longitude\n",
    "fact_table = spark.sql('''\n",
    "    SELECT  dfi.i94yr as year\n",
    "            ,dfi.i94mon as month\n",
    "            ,dfi.i94cit as city\n",
    "            ,dfi.i94port as i94port\n",
    "            ,dfi.arrdate as arrival_date\n",
    "            ,dfi.depdate as departure_date\n",
    "            ,dfi.i94visa as reason\n",
    "            ,dft.AverageTemperature as temperature\n",
    "            ,dft.Latitude as latitude\n",
    "            ,dft.Longitude as longitude\n",
    "    FROM    df_immigration dfi\n",
    "            JOIN df_temperature dft\n",
    "                ON dfi.i94port = dft.i94port\n",
    "''')\n",
    "\n",
    "# write fact_table results (partitioned by i94port) in parquet format\n",
    "# complete path: /results/fact.parquet\n",
    "fact_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"results/fact.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def quality_check(df, description):\n",
    "    '''\n",
    "    This function is aimed at data quality.\n",
    "    Counts the number of rows in the dataframe.\n",
    "    \n",
    "    Input: \n",
    "    * df the dataframe\n",
    "    * description the dataframe descripcion\n",
    "    '''\n",
    "    \n",
    "    # check if the dataframe has data or not\n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(f\"Data quality check failed for {description} with zero records\")\n",
    "    else:\n",
    "        print(f\"Data quality check passed for {description} with {result} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for immigration table with 3088544 records\n",
      "Data quality check passed for temperature table with 207 records\n"
     ]
    }
   ],
   "source": [
    "# verifies the data quality for the tables of: immigration and temperature\n",
    "quality_check(df_immigration, \"immigration table\")\n",
    "quality_check(df_temperature, \"temperature table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Fact Table** - This will contain information from the I94 immigration data joined with the city temperature data on i94port\n",
    "\n",
    "Columns:\n",
    "- i94yr = 4 digit year\n",
    "- i94mon = numeric month\n",
    "- i94cit = 3 digit code of origin city\n",
    "- i94port = 3 character code of destination city\n",
    "- arrdate = arrival date\n",
    "- i94mode = 1 digit travel code\n",
    "- depdate = departure date\n",
    "- i94visa = reason for immigration\n",
    "- AverageTemperature = average temperature of destination city\n",
    "\n",
    "**1st Dimension Table** - This will contain events from the I94 immigration data.\n",
    "\n",
    "Columns:\n",
    "- i94yr = 4 digit year\n",
    "- i94mon = numeric month\n",
    "- i94cit = 3 digit code of origin city\n",
    "- i94port = 3 character code of destination city\n",
    "- arrdate = arrival date\n",
    "- i94mode = 1 digit travel code\n",
    "- depdate = departure date\n",
    "- i94visa = reason for immigration\n",
    "\n",
    "\n",
    "**2nd Dimension Table** - This will contain city temperature data.\n",
    "\n",
    "Columns:\n",
    "- i94port = 3 character code of destination city (mapped from cleaned up immigration data)\n",
    "- AverageTemperature = average temperature\n",
    "- City = city name\n",
    "- Country = country name\n",
    "- Latitude= latitude\n",
    "- Longitude = longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project. <br>\n",
    "Spark is being used due to the different format that allows to handle (SAS, CSV, etc).\n",
    "* Propose how often the data should be updated and why. <br>\n",
    "Since the format of the raw files are monthly, we should continue pulling the data monthly.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x. <br>\n",
    " Amazon RedShift could be used. RedShipT is an analyzed database optimized for large workloads.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day. <br>\n",
    " Apache Airflow could be used. It would be scheduled to run the dag every day. If you fail to send an email.\n",
    " * The database needed to be accessed by 100+ people. <br>\n",
    " Amazon Redshift is useful for this task as it has good read performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
